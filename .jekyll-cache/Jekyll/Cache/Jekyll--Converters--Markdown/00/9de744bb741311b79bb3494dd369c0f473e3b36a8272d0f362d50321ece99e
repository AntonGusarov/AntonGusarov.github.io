I"£<p>Today consider the standard concepts in machine learning: <strong>overfitting</strong> and <strong>underfitting</strong>, its relation to the <strong>model complexity</strong> that are all integrated in the <strong>bias-variace tradeoff</strong>.</p>

<p>The first is <strong>overfitting</strong> - a phenomenon when the error on the test sample is noticeably greater than the error on the train one. This is the main problem of machine learning: if there was no such effect i.e. the error on the test almost coincided with the error on the training, then all training would be just reduced to minimizing the error on the test data  (so-called empirical risk) which doesnâ€™t apparently happen.</p>

<p>In [1], seven properties are proposed validation metrics ideally should have. These properties form a general framework for the development of new metrics depending on the subject area and modeling goals. In particular, any metric must possess the metric properties in the mathematical sense as a measure of the residual: 
\[d(x, y) \geq 0 \]
\[d(x, y)=d(y, x)\]
\[d(x, y)+d(y, z) \geq d(x, z)\]
\[d(x, y)=0 \Leftrightarrow x=y\]</p>

<h1 id="1-vector-norms">1. Vector norms</h1>

:ET